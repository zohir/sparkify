{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from os.path import abspath\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql.types import  IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, Normalizer\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, round, first, avg, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "import time, datetime\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-colors",
   "metadata": {},
   "source": [
    "# Define the user dataset to load and the evaluation period \n",
    " <a id='home'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_size = 'giga'\n",
    "\n",
    "\n",
    "\n",
    "evaluation_period_config = {\n",
    "    \"tiny\" : { # the last one day of the 3 days logs\n",
    "        \"start_date_str\" : \"21/12/2017\",\n",
    "        \"end_date_str\" : \"22/12/2018\",\n",
    "   },\n",
    "    \"mini\" : { # the last two weeks of the 2 months logs\n",
    "        \"start_date_str\" : \"20/11/2018\",\n",
    "        \"end_date_str\" : \"04/12/2018\",\n",
    "   },\n",
    "    \"giga\" : { # the last two weeks of the 2 months logs\n",
    "        \"start_date_str\" : \"20/11/2018\",\n",
    "        \"end_date_str\" : \"04/12/2018\",\n",
    "   }\n",
    "}\n",
    "\n",
    "evaluation_start_date_str = evaluation_period_config[log_file_size][\"start_date_str\"]\n",
    "evaluation_start_dt = datetime.datetime.strptime(evaluation_start_date_str,\"%d/%m/%Y\") \n",
    "evaluation_start_tms = int(datetime.datetime.timestamp(evaluation_start_dt)) * 1000\n",
    "\n",
    "\n",
    "user_log_table = log_file_size+'_user_logs'\n",
    "user_data_table = log_file_size+'_user_data_'+evaluation_start_date_str.replace('/','_')\n",
    "\n",
    "\n",
    "def showNice(sdf, n=5):\n",
    "    return pd.DataFrame(sdf.take(n), columns=sdf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "placed-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start or get a Spark session with Hive support\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sparkify-features-\"+log_file_size) \\\n",
    "    .config(\"spark.sql.warehouse.dir\", abspath('spark-warehouse')) \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-commission",
   "metadata": {},
   "source": [
    "# Load user data from Hive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "future-campaign",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user dataFrame from table giga_user_data_20_11_2018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>max_days_before_eval</th>\n",
       "      <th>active_days</th>\n",
       "      <th>avg_nb_session_per_day</th>\n",
       "      <th>avg_length_per_day</th>\n",
       "      <th>avg_length_per_session</th>\n",
       "      <th>sum_nb_NextSong</th>\n",
       "      <th>avg_nb_NextSong</th>\n",
       "      <th>avg_pct_NextSong</th>\n",
       "      <th>sum_nb_ThumbsUp</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_nb_Error</th>\n",
       "      <th>avg_pct_Error</th>\n",
       "      <th>sum_nb_SubmitDowngrade</th>\n",
       "      <th>avg_nb_SubmitDowngrade</th>\n",
       "      <th>avg_pct_SubmitDowngrade</th>\n",
       "      <th>sum_nb_logs</th>\n",
       "      <th>avg_nb_logs</th>\n",
       "      <th>avg_pct_logs</th>\n",
       "      <th>churn</th>\n",
       "      <th>avg_active_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169050</td>\n",
       "      <td>50</td>\n",
       "      <td>41</td>\n",
       "      <td>1.487805</td>\n",
       "      <td>31062.292683</td>\n",
       "      <td>24612.613821</td>\n",
       "      <td>5139</td>\n",
       "      <td>125.341463</td>\n",
       "      <td>0.821971</td>\n",
       "      <td>274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02439</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>6147</td>\n",
       "      <td>149.926829</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1105878</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>26677.750000</td>\n",
       "      <td>22427.400000</td>\n",
       "      <td>4279</td>\n",
       "      <td>106.975000</td>\n",
       "      <td>0.831642</td>\n",
       "      <td>191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>5051</td>\n",
       "      <td>126.275000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  max_days_before_eval  active_days  avg_nb_session_per_day  \\\n",
       "0  1169050                    50           41                1.487805   \n",
       "1  1105878                    50           40                1.350000   \n",
       "\n",
       "   avg_length_per_day  avg_length_per_session  sum_nb_NextSong  \\\n",
       "0        31062.292683            24612.613821             5139   \n",
       "1        26677.750000            22427.400000             4279   \n",
       "\n",
       "   avg_nb_NextSong  avg_pct_NextSong  sum_nb_ThumbsUp  ...  avg_nb_Error  \\\n",
       "0       125.341463          0.821971              274  ...      0.121951   \n",
       "1       106.975000          0.831642              191  ...      0.100000   \n",
       "\n",
       "   avg_pct_Error  sum_nb_SubmitDowngrade  avg_nb_SubmitDowngrade  \\\n",
       "0       0.000509                       1                 0.02439   \n",
       "1       0.000510                       1                 0.02500   \n",
       "\n",
       "   avg_pct_SubmitDowngrade  sum_nb_logs  avg_nb_logs  avg_pct_logs  churn  \\\n",
       "0                 0.000400         6147   149.926829           1.0      0   \n",
       "1                 0.000115         5051   126.275000           1.0      1   \n",
       "\n",
       "   avg_active_days  \n",
       "0             0.82  \n",
       "1             0.80  \n",
       "\n",
       "[2 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Users table to DF and define churn label\n",
    "\n",
    "print(f\"Loading user dataFrame from table {user_data_table}\")\n",
    "df_user_data = spark.sql(\"select * from {}\".format(user_data_table))\n",
    "\n",
    "# Initializing the list of features Of Interest to predict churn of users\n",
    "colOI = ['avg_nb_logs', 'max_days_before_eval', 'avg_nb_session_per_day', 'avg_length_per_day', 'avg_length_per_session']\n",
    "\n",
    "# Preview of the user dataset\n",
    "showNice(df_user_data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-survivor",
   "metadata": {},
   "source": [
    "***\n",
    "# How to Prepare dataframe for ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appropriate-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dfs_4_model(df, features_list):\n",
    "    \n",
    "    cols_to_drop = (\"features\",\"ScaledFeatures\")\n",
    "    df = df.drop(* cols_to_drop)\n",
    "\n",
    "    # Create a vector NumFeatures with numerical features\n",
    "    featureAssembler = VectorAssembler(inputCols=features_list, outputCol=\"features\")\n",
    "    df = featureAssembler.transform(df)\n",
    "\n",
    "    # Scale the features vector \n",
    "    featureScaler = StandardScaler(inputCol=\"features\", outputCol=\"ScaledFeatures\", withStd=True)\n",
    "    featureScalerModel = featureScaler.fit(df)\n",
    "    df = featureScalerModel.transform(df)\n",
    "\n",
    "    df = df.select(col(\"userId\"), col(\"churn\").alias(\"label\"), col(\"features\"), col('ScaledFeatures'))\n",
    "\n",
    "    # Index the label\n",
    "    cols_to_drop = (\"indexedLabel\", \"indexedFeatures\")\n",
    "    df = df.drop(* cols_to_drop)\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)\n",
    "    df = labelIndexer.transform(df)\n",
    "    featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=2).fit(df)\n",
    "    df = featureIndexer.transform(df)\n",
    "\n",
    "    (trainingData, testData) = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    return (trainingData, testData)\n",
    "\n",
    "#(trainingData, testData) = prepare_dfs_4_model(df_user_data, colOI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-dividend",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-wonder",
   "metadata": {},
   "source": [
    "# How to evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rising-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_recall_on_label(df_ml_pred, label):\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(metricName='recallByLabel', metricLabel=label)\n",
    "    score = evaluator_recall.evaluate(df_ml_pred)\n",
    "    return score\n",
    "\n",
    "def get_model_f1(df_ml_pred):\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "    score = evaluator.evaluate(df_ml_pred)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-angel",
   "metadata": {},
   "source": [
    "***\n",
    "# How to execute models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "structured-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model_name = '', details=False):\n",
    "\n",
    "        \n",
    "    score_recall_0 = get_model_recall_on_label(df, 0)\n",
    "    score_recall_1 = get_model_recall_on_label(df, 1)\n",
    "    score_f1 = get_model_f1(df)\n",
    "    ret = {\"score_f1\":score_f1,\"score_recall_0\":score_recall_0, \"score_recall_1\":score_recall_1} \n",
    "\n",
    "    if(details):\n",
    "        print(\"-\"*50, \"\\n\", model_name)\n",
    "        print(f\"Recall ->  on Renovations = {score_recall_0:.2f}, on Cancelations = {score_recall_1:.2f}\")\n",
    "        model_evaluation_df = df.groupBy().agg(\n",
    "            count(col('userId')).alias('nb_users'),\n",
    "            count_if(col('label') == 1).alias('nb_cancellations'),\n",
    "            count_if((col('label') == 1) & (col('prediction') == col('label'))).alias('nb_cancellations_predicted'),\n",
    "            count_if(col('label') == 0).alias('nb_renovations'),\n",
    "            count_if((col('label') == 0) & (col('prediction') == col('label'))).alias('nb_renovations_predicted')\n",
    "        ).withColumn(\n",
    "            'score_f1',lit(score_f1)\n",
    "        ).withColumn(\n",
    "            'score_on_renovations_predicted', \n",
    "            F.round(col('nb_renovations_predicted')/col('nb_renovations'), 2) \n",
    "        ).withColumn(\n",
    "            'score_on_cancellations_predicted', \n",
    "            F.round(col('nb_cancellations_predicted')/col('nb_cancellations'), 2) \n",
    "        )\n",
    "        model_evaluation_df.show()\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-judge",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "played-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - f1 score = 0.86188\n"
     ]
    }
   ],
   "source": [
    "def execute_lr(trainingData, testData, details=False):\n",
    "    lr = LogisticRegression(featuresCol=\"ScaledFeatures\", standardization=False, labelCol=\"label\", \\\n",
    "                            maxIter=100, regParam=0.3, elasticNetParam=0.2, fitIntercept=False, \\\n",
    "                            threshold=0.12)\n",
    "    lr_model = lr.fit(trainingData)\n",
    "    lr_pred = lr_model.transform(testData)\n",
    "\n",
    "    f1_score = get_model_f1(lr_pred)\n",
    "    print('Logistic Regression - f1 score = {:0.5f}'.format(f1_score))\n",
    "    return f1_score\n",
    "    \n",
    "#lr_scores = execute_lr(trainingData, testData, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-swedish",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "encouraging-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier - f1 score = 0.89802\n"
     ]
    }
   ],
   "source": [
    "def execute_rf(trainingData, testData, details=False):\n",
    "\n",
    "    randomForestClassifier = RandomForestClassifier(numTrees=10, \\\n",
    "                                                  featuresCol='ScaledFeatures' )\n",
    "    rfc_model = randomForestClassifier.fit(trainingData)\n",
    "    rfc_pred = rfc_model.transform(testData)\n",
    "\n",
    "    f1_score = get_model_f1(rfc_pred)\n",
    "    print('Random Forest Classifier - f1 score = {:0.5f}'.format(f1_score))\n",
    "    return f1_score\n",
    "\n",
    "#rf_scores = execute_rf(trainingData, testData, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-provincial",
   "metadata": {},
   "source": [
    "## Gradient-boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fuzzy-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-boosted tree - f1 score = 0.89939\n"
     ]
    }
   ],
   "source": [
    "def execute_gbt(trainingData, testData, details=False):\n",
    "\n",
    "    gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"ScaledFeatures\")\n",
    "    gbt_model = gbt.fit(trainingData)\n",
    "    gbt_pred = gbt_model.transform(testData)\n",
    "\n",
    "    f1_score = get_model_f1(gbt_pred)\n",
    "    print('Gradient-boosted tree - f1 score = {:0.5f}'.format(f1_score))\n",
    "    return f1_score\n",
    "    \n",
    "gbt_scores = execute_gbt(trainingData, trainingData, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-filter",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-parameter",
   "metadata": {},
   "source": [
    "# List of features from the step 1 - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "latest-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "colOI = ['active_days', 'avg_nb_session_per_day', 'avg_length_per_day', 'avg_length_per_session', 'max_days_before_eval', 'sum_nb_NextSong', 'avg_nb_NextSong',  'sum_nb_ThumbsUp', 'avg_nb_ThumbsUp', 'sum_nb_AddtoPlaylist', \n",
    "'avg_nb_AddtoPlaylist', 'sum_nb_AddFriend', 'avg_nb_AddFriend', 'sum_nb_Downgrade', 'avg_nb_Downgrade', 'sum_nb_ThumbsDown', \n",
    "'avg_nb_ThumbsDown', 'sum_nb_Help', 'avg_nb_Help', 'sum_nb_SaveSettings', 'avg_nb_SaveSettings', 'sum_nb_Error', 'avg_nb_Error',\n",
    "'sum_nb_SubmitDowngrade', 'avg_nb_SubmitDowngrade', 'avg_nb_logs', 'sum_nb_logs', \n",
    "'avg_pct_NextSong', 'avg_pct_ThumbsUp', 'avg_pct_AddtoPlaylist', 'avg_pct_AddFriend', 'avg_pct_ThumbsDown', 'avg_pct_Help', 'avg_pct_SaveSettings', 'avg_pct_Error'  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-duncan",
   "metadata": {},
   "source": [
    "***\n",
    "# Evaluate features by correlation score \n",
    "We will calculate the correlation score between the features and the churn label.  \n",
    "We will mark as relevant all the features having a correlation with churn superior to 0.05 or inferior to 0.05 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "present-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_with_churn(df):\n",
    "    corr_features = {}\n",
    "    relevant_features = [] \n",
    "\n",
    "    df_users_sample_ids = ['userId', 'churn']\n",
    "\n",
    "    list_features = [f for f in df.columns if f not in df_users_sample_ids]\n",
    "    for f in list_features:\n",
    "        corr_value = df.stat.corr(\"churn\", f)\n",
    "        corr_features[f] = corr_value \n",
    "        if(abs(corr_value) > 0.05):\n",
    "            relevant_features.append(f)\n",
    "    \n",
    "    return corr_features, relevant_features\n",
    "\n",
    "corr_feature_exp0, relevant_features = get_correlation_with_churn(df_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "identified-sociology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation score for each features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>active_days</th>\n",
       "      <td>0.257317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_active_days</th>\n",
       "      <td>0.251368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_NextSong</th>\n",
       "      <td>0.245814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_Downgrade</th>\n",
       "      <td>0.239975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_ThumbsDown</th>\n",
       "      <td>0.239476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_AddtoPlaylist</th>\n",
       "      <td>0.237141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_AddFriend</th>\n",
       "      <td>0.236340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_ThumbsUp</th>\n",
       "      <td>0.208192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_SaveSettings</th>\n",
       "      <td>0.188833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_NextSong</th>\n",
       "      <td>0.125530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_length_per_day</th>\n",
       "      <td>0.125264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_Downgrade</th>\n",
       "      <td>0.118778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_SubmitDowngrade</th>\n",
       "      <td>0.110942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_AddtoPlaylist</th>\n",
       "      <td>0.107699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_length_per_session</th>\n",
       "      <td>0.097457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_AddFriend</th>\n",
       "      <td>0.087594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_NextSong</th>\n",
       "      <td>0.085617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_days_before_eval</th>\n",
       "      <td>0.085281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_ThumbsUp</th>\n",
       "      <td>0.080444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_ThumbsDown</th>\n",
       "      <td>0.075047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_logs</th>\n",
       "      <td>0.059130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_Downgrade</th>\n",
       "      <td>0.054511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_logs</th>\n",
       "      <td>0.047354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_Error</th>\n",
       "      <td>0.046293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_SaveSettings</th>\n",
       "      <td>0.032548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_AddtoPlaylist</th>\n",
       "      <td>0.022687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_Error</th>\n",
       "      <td>0.021178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_nb_Help</th>\n",
       "      <td>0.007718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_AddFriend</th>\n",
       "      <td>0.006603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_Help</th>\n",
       "      <td>0.006176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_SubmitDowngrade</th>\n",
       "      <td>0.005578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_ThumbsDown</th>\n",
       "      <td>0.003743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_SubmitDowngrade</th>\n",
       "      <td>0.003662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_Help</th>\n",
       "      <td>0.002990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_nb_session_per_day</th>\n",
       "      <td>-0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_Error</th>\n",
       "      <td>-0.004904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_SaveSettings</th>\n",
       "      <td>-0.006853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_ThumbsUp</th>\n",
       "      <td>-0.009540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pct_logs</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             corr\n",
       "active_days              0.257317\n",
       "avg_active_days          0.251368\n",
       "sum_nb_NextSong          0.245814\n",
       "sum_nb_Downgrade         0.239975\n",
       "sum_nb_ThumbsDown        0.239476\n",
       "sum_nb_AddtoPlaylist     0.237141\n",
       "sum_nb_AddFriend         0.236340\n",
       "sum_nb_ThumbsUp          0.208192\n",
       "sum_nb_SaveSettings      0.188833\n",
       "avg_nb_NextSong          0.125530\n",
       "avg_length_per_day       0.125264\n",
       "avg_nb_Downgrade         0.118778\n",
       "sum_nb_SubmitDowngrade   0.110942\n",
       "avg_nb_AddtoPlaylist     0.107699\n",
       "avg_length_per_session   0.097457\n",
       "avg_nb_AddFriend         0.087594\n",
       "avg_pct_NextSong         0.085617\n",
       "max_days_before_eval     0.085281\n",
       "avg_nb_ThumbsUp          0.080444\n",
       "avg_nb_ThumbsDown        0.075047\n",
       "sum_nb_logs              0.059130\n",
       "avg_pct_Downgrade        0.054511\n",
       "avg_nb_logs              0.047354\n",
       "sum_nb_Error             0.046293\n",
       "avg_nb_SaveSettings      0.032548\n",
       "avg_pct_AddtoPlaylist    0.022687\n",
       "avg_nb_Error             0.021178\n",
       "sum_nb_Help              0.007718\n",
       "avg_pct_AddFriend        0.006603\n",
       "avg_nb_Help              0.006176\n",
       "avg_nb_SubmitDowngrade   0.005578\n",
       "avg_pct_ThumbsDown       0.003743\n",
       "avg_pct_SubmitDowngrade  0.003662\n",
       "avg_pct_Help             0.002990\n",
       "avg_nb_session_per_day  -0.000943\n",
       "avg_pct_Error           -0.004904\n",
       "avg_pct_SaveSettings    -0.006853\n",
       "avg_pct_ThumbsUp        -0.009540\n",
       "avg_pct_logs                  NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Correlation score for each features\")\n",
    "df_features_corr = pd.DataFrame.from_dict(corr_feature_exp0, orient='index', columns=['corr']).sort_values(by='corr', ascending=False)\n",
    "df_features_corr.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "religious-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " List of features with more than 0.05 or less than -0.05 \n",
      "We defined 22 relevant features for churn prediction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['max_days_before_eval',\n",
       " 'active_days',\n",
       " 'avg_length_per_day',\n",
       " 'avg_length_per_session',\n",
       " 'sum_nb_NextSong',\n",
       " 'avg_nb_NextSong',\n",
       " 'avg_pct_NextSong',\n",
       " 'sum_nb_ThumbsUp',\n",
       " 'avg_nb_ThumbsUp',\n",
       " 'sum_nb_AddtoPlaylist',\n",
       " 'avg_nb_AddtoPlaylist',\n",
       " 'sum_nb_AddFriend',\n",
       " 'avg_nb_AddFriend',\n",
       " 'sum_nb_Downgrade',\n",
       " 'avg_nb_Downgrade',\n",
       " 'avg_pct_Downgrade',\n",
       " 'sum_nb_ThumbsDown',\n",
       " 'avg_nb_ThumbsDown',\n",
       " 'sum_nb_SaveSettings',\n",
       " 'sum_nb_SubmitDowngrade',\n",
       " 'sum_nb_logs',\n",
       " 'avg_active_days']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n List of features with more than 0.05 or less than -0.05 \")\n",
    "print(\"We defined {} relevant features for churn prediction\".format(len(relevant_features)))\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-progress",
   "metadata": {},
   "source": [
    "***\n",
    "# Split the dataset and fit some ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "complimentary-america",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - f1 score = 0.86271\n",
      "Random Forest Classifier - f1 score = 0.89360\n",
      "Gradient-boosted tree - f1 score = 0.89520\n",
      "CPU times: user 91.7 ms, sys: 25.7 ms, total: 117 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pack the data and train ML\n",
    "\n",
    "# Features list is colOI or relevant_features\n",
    "\n",
    "(trainingData, testData) = prepare_dfs_4_model(df_user_data, colOI)\n",
    "trainingData.persist(StorageLevel.MEMORY_ONLY)\n",
    "testData.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "lr_scores = execute_lr(trainingData, testData, True)\n",
    "rf_scores = execute_rf(trainingData, testData, True)\n",
    "gbt_scores = execute_gbt(trainingData, testData, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-rings",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "happy-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      " Gradient-boosted tree\n",
      "Recall ->  on Renovations = 1.00, on Cancelations = 0.02\n",
      "+--------+----------------+--------------------------+--------------+------------------------+-----------------+------------------------------+--------------------------------+\n",
      "|nb_users|nb_cancellations|nb_cancellations_predicted|nb_renovations|nb_renovations_predicted|         score_f1|score_on_renovations_predicted|score_on_cancellations_predicted|\n",
      "+--------+----------------+--------------------------+--------------+------------------------+-----------------+------------------------------+--------------------------------+\n",
      "|    4482|             307|                         5|          4175|                    4167|0.900237754323376|                           1.0|                            0.02|\n",
      "+--------+----------------+--------------------------+--------------+------------------------+-----------------+------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def execute_gbt(trainingData, testData, details=False):\n",
    "\n",
    "    gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "    gbt_model = gbt.fit(trainingData)\n",
    "    gbt_pred = gbt_model.transform(testData)\n",
    "\n",
    "    gbt_score = evaluate_model(gbt_pred, 'Gradient-boosted tree', details)\n",
    "\n",
    "# Features list is relevant_features | colOI\n",
    "# The gbt model give better results with the full list of available features \n",
    "(trainingData, testData) = prepare_dfs_4_model(df_user_data, colOI)\n",
    "trainingData.persist(StorageLevel.MEMORY_ONLY)\n",
    "testData.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "gbt_scores = execute_gbt(trainingData, testData, True)\n",
    "gbt_scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "revised-experience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.78 s, sys: 1.88 s, total: 8.65 s\n",
      "Wall time: 1h 45min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8918544273595881,\n",
       " 0.8918544273595881,\n",
       " 0.8914420162446484,\n",
       " 0.8914706288259926,\n",
       " 0.8922723774738587,\n",
       " 0.8922723774738587,\n",
       " 0.8918029239834274,\n",
       " 0.8918032272560891,\n",
       " 0.8921767039385688,\n",
       " 0.8924505316123161,\n",
       " 0.8923447415399302,\n",
       " 0.8928969002330915,\n",
       " 0.8930532838161447,\n",
       " 0.893056637196646,\n",
       " 0.893946677696132,\n",
       " 0.8936308315636147,\n",
       " 0.8923964656198871,\n",
       " 0.8926428925822978,\n",
       " 0.8936084995864642,\n",
       " 0.8936697001937832,\n",
       " 0.8931527306956744,\n",
       " 0.8928365304563272,\n",
       " 0.8933381394731447,\n",
       " 0.8938742379298663,\n",
       " 0.8881898635747638,\n",
       " 0.8898917212110552,\n",
       " 0.8893753976510401,\n",
       " 0.8913121901285856,\n",
       " 0.8888297880962944,\n",
       " 0.8905626443726189,\n",
       " 0.8895220369517398,\n",
       " 0.8923345886940921,\n",
       " 0.889053266251377,\n",
       " 0.8911584745833113,\n",
       " 0.8912674551853655,\n",
       " 0.8924206703977313]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"ScaledFeatures\", maxIter=10, maxDepth=5)\n",
    "\n",
    "#gbt_model = gbt.fit(trainingData)\n",
    "#gbt_pred = gbt_model.transform(testData)\n",
    "#gbt_score = evaluate_model(gbt_pred, 'Gradient-boosted tree')\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth,[3, 5, 10]) \\\n",
    "    .addGrid(gbt.maxIter,[10, 20, 50]) \\\n",
    "    .addGrid(gbt.maxBins,[10, 30]) \\\n",
    "    .addGrid(gbt.minInstancesPerNode,[1, 2]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# Default model hyper params\n",
    "\"\"\"\n",
    "cacheNodeIds = False\n",
    "checkpointInterval = 10\n",
    "featureSubsetStrategy = all\n",
    "featuresCol = ScaledFeatures\n",
    "impurity = variance\n",
    "labelCol = indexedLabel\n",
    "leafCol = \n",
    "lossType = logistic\n",
    "maxBins = 32\n",
    "maxDepth = 5 -\n",
    "maxIter = 10 -\n",
    "maxMemoryInMB = 256\n",
    "minInfoGain = 0.3 -\n",
    "minInstancesPerNode = 1 -\n",
    "minWeightFractionPerNode = 0.0\n",
    "predictionCol = prediction\n",
    "probabilityCol = probability\n",
    "rawPredictionCol = rawPrediction\n",
    "seed = 6596411224379334233\n",
    "stepSize = 0.1\n",
    "subsamplingRate = 1.0\n",
    "validationTol = 0.01\n",
    "\"\"\"\n",
    "\n",
    "# Hyper params found on CV Best model \n",
    "\"\"\" \n",
    "cacheNodeIds = False\n",
    "checkpointInterval = 10\n",
    "featureSubsetStrategy = all\n",
    "featuresCol = ScaledFeatures\n",
    "impurity = variance\n",
    "labelCol = indexedLabel\n",
    "leafCol = \n",
    "lossType = logistic\n",
    "maxBins = 30\n",
    "maxDepth = 5 -\n",
    "maxIter = 10 -\n",
    "maxMemoryInMB = 256\n",
    "minInfoGain = 0.0 -\n",
    "minInstancesPerNode = 1 -\n",
    "minWeightFractionPerNode = 0.0\n",
    "predictionCol = prediction\n",
    "probabilityCol = probability\n",
    "rawPredictionCol = rawPrediction\n",
    "seed = -46955970000844093\n",
    "stepSize = 0.1\n",
    "subsamplingRate = 1.0\n",
    "validationTol = 0.01\"\"\"\n",
    "\n",
    "mcEvaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=mcEvaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "gbt_CV = crossval.fit(trainingData)\n",
    "gbt_CV.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-russia",
   "metadata": {},
   "source": [
    "***\n",
    "# Trying the best model of gbt\n",
    "\n",
    "The model tunning did not give a better result. \n",
    "# default params -> f1 score = 0.89939\n",
    "# tunned params  -> f1 score = 0.89639\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "front-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-boosted tree - f1 score = 0.89639\n"
     ]
    }
   ],
   "source": [
    "gbt_pred = gbt_CV.transform(testData)\n",
    "\n",
    "gbt_best_f1_score = get_model_f1(gbt_pred)\n",
    "print('Gradient-boosted tree - f1 score = {:0.5f}'.format(gbt_best_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "developed-secretariat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances in the best model = (35,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,27,28,29,30,31,32,33,34],[0.21558188665878564,0.020464568465465655,0.025469584623258514,0.019639445945552798,0.017229910980739272,0.020282067494215766,0.01935076023843168,0.0059147919408299625,0.057962037335869486,0.005199492282964488,0.014565248703292727,0.12355143550902146,0.026570095240251636,0.051389356324734334,0.024497676508029285,0.04204847876499084,0.0126373514202659,0.006552639714615697,0.005523651017292248,0.012240217761812212,0.006609498277893386,0.007880995971337726,0.004580372909836257,0.0223304338502896,0.016358348921942226,0.004732483539005413,0.03315981479554837,0.04186724330559259,0.016331308682029172,0.02351920856717982,0.05973962956477063,0.011169024644134952,0.010228233530330907,0.014822706509689329])\n",
      "Hyper parameters of the best model \n",
      " **************************************************\n",
      "cacheNodeIds = False\n",
      "checkpointInterval = 10\n",
      "featureSubsetStrategy = all\n",
      "featuresCol = ScaledFeatures\n",
      "impurity = variance\n",
      "labelCol = indexedLabel\n",
      "leafCol = \n",
      "lossType = logistic\n",
      "maxBins = 30\n",
      "maxDepth = 5\n",
      "maxIter = 10\n",
      "maxMemoryInMB = 256\n",
      "minInfoGain = 0.0\n",
      "minInstancesPerNode = 1\n",
      "minWeightFractionPerNode = 0.0\n",
      "predictionCol = prediction\n",
      "probabilityCol = probability\n",
      "rawPredictionCol = rawPrediction\n",
      "seed = -46955970000844093\n",
      "stepSize = 0.1\n",
      "subsamplingRate = 1.0\n",
      "validationTol = 0.01\n"
     ]
    }
   ],
   "source": [
    "# Looking at the optimal hyper parameters found\n",
    "bestModel_CV = gbt_CV.bestModel\n",
    "evaluator_CV = gbt_CV.getEvaluator() \n",
    "\n",
    "a_GBTClassificationModel = bestModel_CV.stages[0]\n",
    "featureImportances = a_GBTClassificationModel.featureImportances\n",
    "print(f\"Feature Importances in the best model = {featureImportances}\")\n",
    "\n",
    "paramMap = a_GBTClassificationModel.extractParamMap() \n",
    "\n",
    "print(\"Hyper parameters of the best model\", \"\\n\", \"*\"*50)\n",
    "for k, v in paramMap.items():\n",
    "    print(f\"{k.name} = {v}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-provincial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
